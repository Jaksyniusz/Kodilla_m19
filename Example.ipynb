{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71d896-65eb-41b0-9c87-b64993ac208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\" -O \"/tmp/cats-and-dogs.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078af3bb-52db-42f8-93d6-9f5d5ffd75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid(file_path):\n",
    "    correct_files = []\n",
    "    for name in os.listdir(file_path):\n",
    "        try:\n",
    "            img = Image.open(file_path + \"/\" + name)\n",
    "            correct_files.append(name)\n",
    "        except UnidentifiedImageError:\n",
    "            pass\n",
    "    return correct_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf8ad0-4eca-465a-8f35-7ac4e14da2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_zip = '/tmp/cats-and-dogs.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n",
    "classes = ['Cat', 'Dog']\n",
    "original_cat_path = '/tmp/PetImages/Cat'\n",
    "original_dog_path = '/tmp/PetImages/Dog'\n",
    "original_cat = get_valid(original_cat_path)\n",
    "original_dog = get_valid(original_dog_path)\n",
    "random.seed(101)\n",
    "\n",
    "random.shuffle(original_cat)\n",
    "random.shuffle(original_dog)\n",
    "size = min(len(original_cat), len(original_dog))\n",
    "train_size = int(np.floor(0.7 * size))\n",
    "\n",
    "valid_size = int(np.floor(0.2 * size))\n",
    "test_size = size - train_size - valid_size\n",
    "base_directory = 'dataset'\n",
    "os.mkdir(base_directory)\n",
    "type_datasets = ['train', 'valid', 'test']\n",
    "directories = {}\n",
    "\n",
    "for type_dataset in type_datasets:\n",
    "    directory = os.path.join(base_directory, type_dataset)\n",
    "    os.mkdir(directory)\n",
    "    for name_class in classes:\n",
    "        animal = os.path.join(directory, name_class)\n",
    "        os.mkdir(animal)\n",
    "        directories[f'{type_dataset}_{name_class}'] = animal+'/'\n",
    "index = 0\n",
    "\n",
    "for name_cat, name_dog in zip(original_cat, original_dog):\n",
    "        if index <= train_size:\n",
    "            type_of_dataset = 'train'\n",
    "        elif train_size < index <= (train_size + valid_size):\n",
    "            type_of_dataset = 'valid'\n",
    "        elif (train_size + valid_size) < index <= (train_size + valid_size + test_size):\n",
    "            type_of_dataset = 'test'\n",
    "        shutil.copyfile(src=(original_cat_path + '/' +name_cat), dst=(directories[f'{type_of_dataset}_Cat']+name_cat))\n",
    "        shutil.copyfile(src=(original_dog_path + '/' + name_dog), dst=(directories[f'{type_of_dataset}_Dog']+name_dog))\n",
    "        index += 1\n",
    "\n",
    "print(f'Dog - train: {len(os.listdir(directories[\"train_Dog\"]))}\\tCat - train: {len(os.listdir(directories[\"train_Cat\"]))}')\n",
    "print(f'Dog - valid: {len(os.listdir(directories[\"valid_Dog\"]))}\\tCat - valid: {len(os.listdir(directories[\"valid_Cat\"]))}')\n",
    "print(f'Dog - test:  {len(os.listdir(directories[\"test_Dog\"]))}\\tCat - test:  {len(os.listdir(directories[\"test_Cat\"]))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d3596-593c-4768-92cb-4460ee732142",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 12))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "for i, element in enumerate(os.listdir(os.path.join(directories[\"train_Cat\"]))[:8]):\n",
    "    ax = fig.add_subplot(4, 4, i+1)\n",
    "    img = Image.open(directories[\"train_Cat\"]+element)\n",
    "    ax.imshow(img)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "for i, element in enumerate(os.listdir(os.path.join(directories[\"train_Dog\"]))[:8]):\n",
    "    ax = fig.add_subplot(4, 4, i+9)\n",
    "    img = Image.open(directories[\"train_Dog\"]+element)\n",
    "    ax.imshow(img)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb00968-dbbf-4d99-8c49-2ab8edc239eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 150, 150\n",
    "train_data_dir = 'dataset/train/'\n",
    "validation_data_dir = 'dataset/valid/'\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "steps_per_epoch = train_size // batch_size\n",
    "validation_steps = valid_size // batch_size\n",
    "patience = 5\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    batch_size=batch_size, class_mode='binary')\n",
    "validation_generator = test_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size, class_mode='binary')\n",
    "train_datagen_augmentation = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   rotation_range=30,\n",
    "                                   horizontal_flip=True)\n",
    "train_generator_augmentation = train_datagen_augmentation.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(img_height, img_width),\n",
    "                                                    batch_size=batch_size, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186df329-c0b7-49b7-8235-9ced335acbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "os.mkdir(\"history\")\n",
    "os.mkdir(\"charts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19606b-1b5f-43a2-a041-70db5eaa15f9",
   "metadata": {},
   "source": [
    "### Model podstawowy - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e7f7b-c971-4304-9b20-7abeb74d4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = Sequential()\n",
    "model_baseline.add(Flatten(input_shape=(150, 150, 3)))\n",
    "model_baseline.add(Dense(units=1, activation='sigmoid'))\n",
    "model_baseline.compile(loss='binary_crossentropy',\n",
    "                       optimizer=RMSprop(learning_rate=1e-4),\n",
    "                       metrics=['accuracy'])\n",
    "model_baseline.summary()\n",
    "models.append(\"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df12b86-cea6-4741-9896-49a0c9017d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=patience, monitor='val_accuracy', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d6ff3-1865-449a-a7ad-f58476ae1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_baseline = model_baseline.fit_generator(train_generator,\n",
    "                                                steps_per_epoch=steps_per_epoch,\n",
    "                                                epochs=epochs,\n",
    "                                                validation_data=validation_generator,\n",
    "                                                validation_steps=validation_steps,\n",
    "                                                callbacks=[es])\n",
    "history_baseline_df = pd.DataFrame(history_baseline.history)\n",
    "history_baseline_csv_file = 'history/history_baseline.csv'\n",
    "\n",
    "with open(history_baseline_csv_file, mode='w') as f:\n",
    "    history_baseline_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724507b6-3e8b-4727-8eb9-c0fea90351db",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = 0\n",
    "min_accuracy = 1\n",
    "max_loss = 0\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(models)))\n",
    "\n",
    "for model in models:\n",
    "    df = pd.read_csv(f'history/history_{model}.csv', index_col=0)\n",
    "    df.index += 1\n",
    "    if max_index < max(df.index):\n",
    "        max_index = max(df.index)\n",
    "    if min_accuracy > min(df[['accuracy', 'val_accuracy']].min()):\n",
    "        min_accuracy = min(df[['accuracy', 'val_accuracy']].min())\n",
    "    if max_loss < max(df[['loss', 'val_loss']].max()):\n",
    "        max_loss = max(df[['loss', 'val_loss']].max())\n",
    "\n",
    "for model in models:\n",
    "    df = pd.read_csv(f'history/history_{model}.csv', index_col=0)\n",
    "    df.index += 1\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    ax = fig.add_subplot(211)\n",
    "    ax.plot(df['accuracy'], \"bp--\")\n",
    "    ax.plot(df['val_accuracy'], \"rp--\")\n",
    "    ax.set_title(f'Model {model} Accuracy', fontsize=20)\n",
    "    ax.set_ylabel('Accuracy', fontsize=15)\n",
    "    ax.set_xlabel('Epoch', fontsize=15)\n",
    "    ax.set_xlim([1, max_index])\n",
    "    ax.set_ylim([min_accuracy, 1])\n",
    "\n",
    "    for milestone in (0.7, 0.8, 0.9, 0.95):\n",
    "        ax.axhline(milestone, color=\"k\", linestyle=\"--\")\n",
    "        try:\n",
    "            if min(df[df['val_accuracy'] >= milestone].index) > 1:\n",
    "                plt.axvline(min(df[df['val_accuracy'] >= milestone].index), color=\"g\", linestyle=\"--\")\n",
    "                ax.text(min(df[df['val_accuracy'] >= milestone].index)+0.6, min_accuracy+0.02,\n",
    "                        f'Epoch: {min(df[df[\"val_accuracy\"] >= milestone].index)}', rotation=90)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    ax = fig.add_subplot(212)\n",
    "    ax.plot(df['loss'], \"bp--\")\n",
    "    ax.plot(df['val_loss'], \"rp--\")\n",
    "    ax.set_title(f'Model {model} Loss', fontsize=20)\n",
    "    ax.set_ylabel('Loss', fontsize=15)\n",
    "    ax.set_xlabel('Epoch', fontsize=15)\n",
    "    ax.set_xlim([1, max_index])\n",
    "    ax.set_ylim([0, max_loss])\n",
    "    ax.legend(['Training', 'Validation'], loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'charts/train_history_{model}.png', transparent=True, dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = fig.add_subplot(211)\n",
    "for model, color in zip(models, colors):\n",
    "    df = pd.read_csv(f'history/history_{model}.csv', index_col=0)\n",
    "    df.index += 1\n",
    "    ax.plot(df['val_accuracy'], label=f'Model {model}', color=color, linewidth=3)\n",
    "    ax.axhline(df['val_accuracy'].max(), color=color, linestyle=\"dotted\", linewidth=4)\n",
    "\n",
    "ax.set_title(f'Accuracy', fontsize=20)\n",
    "ax.set_ylabel('Accuracy', fontsize=15)\n",
    "ax.set_xlabel('Epoch', fontsize=15)\n",
    "ax.set_xlim([1, max_index])\n",
    "ax.set_ylim([min_accuracy, 1])\n",
    "for milestone in (0.7, 0.8, 0.9, 0.95):\n",
    "    ax.axhline(milestone, color=\"k\", linestyle=\"--\")\n",
    "plt.legend(loc='lower right')\n",
    "ax = fig.add_subplot(212)\n",
    "for model, color in zip(models, colors):\n",
    "    df = pd.read_csv(f'history/history_{model}.csv', index_col=0)\n",
    "    df.index += 1\n",
    "    ax.plot(df['val_loss'], label=f'Model {model}', color=color, linewidth=3)\n",
    "    ax.axhline(df['val_loss'].min(), color=color, linestyle=\"dotted\", linewidth=4)\n",
    "ax.set_title(f'Loss', fontsize=20)\n",
    "ax.set_ylabel('Loss', fontsize=15)\n",
    "ax.set_xlabel('Epoch', fontsize=15)\n",
    "ax.set_xlim([1, max_index])\n",
    "ax.set_ylim([0, max_loss])\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'charts/train_history_of_each_model.png', transparent=True, dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6437690-1a85-4c22-8b0f-c1bbf9aa8143",
   "metadata": {},
   "source": [
    "### Model podstawowy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb700423-6cf3-443b-90ce-bca9d3721c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_1 = Sequential()\n",
    "model_simple_1.add(Conv2D(filters=10, kernel_size=(3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model_simple_1.add(MaxPooling2D(2, 2))\n",
    "model_simple_1.add(Flatten())\n",
    "model_simple_1.add(Dense(units=1, activation='sigmoid'))\n",
    "model_simple_1.compile(loss='binary_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])\n",
    "model_simple_1.summary()\n",
    "models.append(\"simple_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c2882-ed6a-4d12-abe0-a678c269b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_1 = model_simple_1.fit_generator(train_generator,\n",
    "                                                steps_per_epoch=steps_per_epoch,\n",
    "                                                epochs=epochs,\n",
    "                                                validation_data=validation_generator,\n",
    "                                                validation_steps=validation_steps,\n",
    "                                                callbacks=[es])\n",
    "\n",
    "history_simple_1_df = pd.DataFrame(history_simple_1.history)\n",
    "history_simple_1_csv_file = 'history/history_simple_1.csv'\n",
    "\n",
    "with open(history_simple_1_csv_file, mode='w') as f:\n",
    "    history_simple_1_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eefa5a-fb1b-455a-b4f2-a1b5beedee83",
   "metadata": {},
   "source": [
    "### Model podstawowy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8d596-8c75-4675-aa93-88aa51367458",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_2 = Sequential()\n",
    "model_simple_2.add(Conv2D(filters=10, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(150, 150, 3)))\n",
    "model_simple_2.add(MaxPooling2D(2, 2))\n",
    "model_simple_2.add(Flatten())\n",
    "model_simple_2.add(Dense(units=1, activation='sigmoid'))\n",
    "model_simple_2.compile(loss='binary_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])\n",
    "model_simple_2.summary()\n",
    "models.append(\"simple_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e01d84-1098-4730-a34b-310c9c107141",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac1fd7-a188-46a8-a5ab-fafe2d7474a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "\n",
    "# Block 1\n",
    "model_1.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(150,150,3)))\n",
    "model_1.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 2\n",
    "model_1.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 3\n",
    "model_1.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 4\n",
    "model_1.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(256, activation='relu'))\n",
    "model_1.add(Dense(128, activation='relu'))\n",
    "model_1.add(Dense(units=1, activation='sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy',\n",
    "                       optimizer=RMSprop(lr=1e-4),\n",
    "                       metrics=['accuracy'])\n",
    "model_1.summary()\n",
    "models.append(\"model_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61302dd9-d1b3-490e-b5c4-b00b052da71a",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab5f67-f399-453c-9559-a78b2d9db416",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "# Block 1\n",
    "model_2.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(150,150,3)))\n",
    "model_2.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 2\n",
    "model_2.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 3\n",
    "model_2.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 4\n",
    "model_2.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(256, activation='relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(128, activation='relu'))\n",
    "model_2.add(Dense(units=1, activation='sigmoid'))\n",
    "model_2.compile(loss='binary_crossentropy',\n",
    "                       optimizer=RMSprop(lr=1e-4),\n",
    "                       metrics=['accuracy'])\n",
    "model_2.summary()\n",
    "models.append(\"model_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87fe12-0bb3-4e45-99f0-793badfcc825",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378b58a-e92c-481a-bb92-07bb3de93e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "\n",
    "# Block 1\n",
    "model_3.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(150,150,3)))\n",
    "model_3.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 2\n",
    "model_3.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 3\n",
    "model_3.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Block 4\n",
    "model_3.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_3.add(Flatten())\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(256, activation='relu'))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(128, activation='relu'))\n",
    "model_3.add(Dense(units=1, activation='sigmoid'))\n",
    "model_3.compile(loss='binary_crossentropy',\n",
    "                       optimizer=RMSprop(lr=1e-4),\n",
    "                       metrics=['accuracy'])\n",
    "model_3.summary()\n",
    "models.append(\"model_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9475b64-2170-4c16-9975-07d260057d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model_3 = model_3.fit_generator(train_generator_augmentation,\n",
    "                                        steps_per_epoch=steps_per_epoch,\n",
    "                                        epochs=epochs,\n",
    "                                        validation_data=validation_generator,\n",
    "                                        validation_steps=validation_steps,\n",
    "                                        callbacks=[es])\n",
    "\n",
    "history_model_3_df = pd.DataFrame(history_model_3.history)\n",
    "history_model_3_csv_file = 'history/history_model_3.csv'\n",
    "with open(history_model_3_csv_file, mode='w') as f:\n",
    "    history_model_3_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef79a8-ebd3-44ef-a263-f746a279d636",
   "metadata": {},
   "source": [
    "#### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b2695-75f4-47b1-bce8-9411030c1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "vgg16.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in vgg16.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "for layer in vgg16.layers:\n",
    "    print(f'layer_name: {layer.name:13} trainable: {layer.trainable}')\n",
    "\n",
    "model_4 = Sequential()\n",
    "model_4.add(vgg16)\n",
    "model_4.add(Flatten())\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(256, activation='relu'))\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(128, activation='relu'))\n",
    "model_4.add(Dense(units=1, activation='sigmoid'))\n",
    "model_4.compile(loss='binary_crossentropy',\n",
    "                       optimizer=RMSprop(lr=1e-4),\n",
    "                       metrics=['accuracy'])\n",
    "model_4.summary()\n",
    "models.append(\"model_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe66b5-bde1-486c-91bd-ec456fd2170f",
   "metadata": {},
   "source": [
    "## 3. Warstwy rekurencyjne\n",
    "### Implementacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789899e3-7b5f-4d09-a67c-0695ad61bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przetwarzanie danych\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# przekształcanie – normalizacja danych\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# wizualizacja\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sieci neuronowe\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "\n",
    "# ewaluacja modelu\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebcbcf7-8e67-4dcb-9354-6ad65a4c2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('airline-passengers.csv')\n",
    "dataset['Month'] = pd.to_datetime(dataset['Month'])\n",
    "dataset.set_index(['Month'], inplace=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59062b5-8d9f-451d-a288-dd06a699295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(dataset['Passengers'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ea883-4b95-46c1-8919-3eb757ba20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.70)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "look_back = 3\n",
    "def create_dataset(df, train_size, lback=look_back, scaler_function=scaler):\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df[0:train_size,:].copy(), df[train_size:len(df),:].copy()\n",
    "    train = scaler_function.fit_transform(train)\n",
    "    test = scaler_function.transform(test)\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "    # Tworzenie zbioru treninowego\n",
    "    for i in range(len(train)-lback-1):\n",
    "        a = train[i:(i+lback), 0]\n",
    "        X_train.append(a)\n",
    "        y_train.append(train[i + lback, 0])\n",
    "\n",
    "    # Tworzenie zbioru testowego\n",
    "    for i in range(len(test)-lback-1):\n",
    "        a = test[i:(i+lback), 0]\n",
    "        X_test.append(a)\n",
    "        y_test.append(test[i + lback, 0])\n",
    "\n",
    "    X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "    X_train, X_test = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1])), np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = create_dataset(df=np.array(dataset), train_size=train_size, lback=look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15863876-ada3-47cb-b5be-47f8def74912",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217aa97-eca1-42ad-8c20-f5d135ff23a3",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1bd3e-507d-447b-8da1-8820a339fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Sequential()\n",
    "model_rnn.add(SimpleRNN(5, input_shape=(1, look_back)))\n",
    "model_rnn.add(Dense(1))\n",
    "model_rnn.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_rnn.fit(X_train, y_train, epochs=200, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9684ddb-5e24-4b7a-8643-3d5da3cf38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtests(model, lback=look_back):\n",
    "\n",
    "    # predykcja - train\n",
    "    prediction_train = scaler.inverse_transform(model.predict(X_train))\n",
    "    prediction_train = pd.Series(prediction_train.flatten(),\n",
    "                                index=dataset.index[lback:len(prediction_train)+lback])\n",
    "\n",
    "    # predykcja - test\n",
    "    prediction_test = scaler.inverse_transform(model.predict(X_test))\n",
    "    prediction_test = pd.Series(prediction_test.flatten(),\n",
    "                                index=dataset.index[len(prediction_train)+(2*lback)+1:len(dataset)-1])\n",
    "\n",
    "    # wizualizacja prognozy\n",
    "    plt.figure(figsize=(16,9))\n",
    "    plt.plot(dataset['Passengers'], color='blue', label='True values')\n",
    "    plt.plot(prediction_train, color='green', label='Prediction - Train')\n",
    "    plt.plot(prediction_test, color='red', label='Prediction - Test')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # obliczenie RMSE\n",
    "    rmse = mean_squared_error(dataset.loc[prediction_test.index, :], prediction_test) ** 0.5\n",
    "    print(f'\\nRMSE TEST: {rmse}')\n",
    "\n",
    "backtests(model_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5602ec8-d171-448f-bf63-9ef45aa95a64",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147fcc30-dd3c-4d19-a400-aa5131ddab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(5, input_shape=(1, look_back)))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_lstm.fit(X_train, y_train, epochs=200, batch_size=1, verbose=1)\n",
    "backtests(model_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f685ea-e650-48eb-a5a5-52b0e06838da",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945dd9e-d8e1-411f-9342-d0bd3228730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = Sequential()\n",
    "model_gru.add(GRU(5, input_shape=(1, look_back)))\n",
    "model_gru.add(Dense(1))\n",
    "model_gru.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_gru.fit(X_train, y_train, epochs=200, batch_size=1, verbose=1)\n",
    "backtests(model_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ec595-8116-4cbf-bba4-11b449be2d2f",
   "metadata": {},
   "source": [
    "## 4. Monitorowanie i tuning procesu uczenia modeli z użyciem MLFlow\n",
    "### 1. Instalacja i uruchomienie MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc7c11-005f-4e27-b573-8954c32fe573",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlflow==1.7.2\n",
    "\n",
    "pip install scikit-learn==1.5.2\n",
    "\n",
    "mlflow ui --port 8001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995d4f1-9b4f-4a82-917a-98f503d2171c",
   "metadata": {},
   "source": [
    "### 2. Autologowanie procesu treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873df8aa-1e29-462d-9e28-08b01ea0fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mlflow.autolog() # <- Tutaj właczamy \"magiczny\" proces autologowania\n",
    "\n",
    "# Ładowanie danych\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
    "\n",
    "# Przetwarzanie danych\n",
    "titanic.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "titanic['Age'].fillna(titanic['Age'].median(), inplace=True)\n",
    "titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace=True)\n",
    "titanic = pd.get_dummies(titanic, drop_first=True)\n",
    "\n",
    "X = titanic.drop('Survived', axis=1)\n",
    "y = titanic['Survived']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Podział na zestawy treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Rozpoczęcie nowego eksperymentu\n",
    "mlflow.set_experiment(\"Analiza danych Titanic\")\n",
    "\n",
    "# Definicja hiperparametrów do strojenia\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Inicjalizacja modelu\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Ustawienie GridSearchCV\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Najlepszy model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Przewidywanie i ewaluacja\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e90f59-4d60-4f9e-b543-06dad933de7c",
   "metadata": {},
   "source": [
    "### Co daje funkcja autolog?\n",
    "Funkcja mlflow.autolog() automatycznie rejestruje następujące informacje:\n",
    "- Hiperparametry modelu: Wszystkie hiperparametry użyte do trenowania modelu.\n",
    "- Metryki: Główne metryki wydajności modelu, takie jak dokładność.\n",
    "- Model: Sam model jest zapisywany, co umożliwia późniejsze ładowanie i używanie go bez konieczności ponownego trenowania.\n",
    "- Artefakty: Dodatkowe pliki, takie jak diagramy, wykresy itp., mogą być automatycznie zapisywane, jeśli są generowane w trakcie treningu.\n",
    "### 3. Logowanie datasetów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0aab2f-eb10-4bad-ab5b-95345921b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mlflow.autolog() # <- Tutaj właczamy \"magiczny\" proces autologowania\n",
    "\n",
    "# Ładowanie danych\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
    "\n",
    "# Przetwarzanie danych\n",
    "titanic.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "titanic['Age'].fillna(titanic['Age'].median(), inplace=True)\n",
    "titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace=True)\n",
    "titanic = pd.get_dummies(titanic, drop_first=True)\n",
    "\n",
    "X = titanic.drop('Survived', axis=1)\n",
    "y = titanic['Survived']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Logujemy informację o użytych datasetach\n",
    "mlflow_dataset = mlflow.data.from_pandas(titanic, targets = \"Survived\",\n",
    "                                  name = \"Titanic Dataset\")\n",
    "mlflow.log_input(mlflow_dataset, context = \"training\")\n",
    "\n",
    "# Podział na zestawy treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Rozpoczęcie nowego eksperymentu\n",
    "mlflow.set_experiment(\"Analiza danych Titanic\")\n",
    "\n",
    "# Definicja hiperparametrów do strojenia\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Inicjalizacja modelu\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Ustawienie GridSearchCV\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Najlepszy model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Przewidywanie i ewaluacja\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20381efd-80dc-4412-b845-65d31ceea37d",
   "metadata": {},
   "source": [
    "### 4. Porównywanie modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62add8-3b86-4e9a-98cb-c3ab7ef7f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:8001\")\n",
    "mlflow.autolog()\n",
    "\n",
    "# Ładowanie danych\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
    "\n",
    "# Przetwarzanie danych\n",
    "titanic.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "titanic['Age'].fillna(titanic['Age'].median(), inplace=True)\n",
    "titanic['Embarked'].fillna(titanic['Embarked'].mode()[0], inplace=True)\n",
    "titanic = pd.get_dummies(titanic, drop_first=True)\n",
    "int_columns = titanic.select_dtypes(include='int').columns\n",
    "titanic[int_columns] = titanic[int_columns].astype(float)\n",
    "\n",
    "X = titanic.drop('Survived', axis=1)\n",
    "y = titanic['Survived'].to_numpy()\n",
    "\n",
    "\n",
    "# Podział na zestawy treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Rozpoczęcie nowego eksperymentu\n",
    "mlflow.set_experiment(\"Analiza danych Titanic\")\n",
    "\n",
    "# Definicja modelu Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Definicja siatki hiperparametrów do przeszukiwania\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Konfiguracja GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Najlepszy mode\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Przewidywanie i ewaluacja\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb586bfb-4fc2-42ac-b591-0415157b0b54",
   "metadata": {},
   "source": [
    "### 5. Ćwiczenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa1937-00a1-4ea3-8dc2-b8f864ba35d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Definicja hiperparametrów\n",
    "num_words = 5000  # Liczba słów w naszym słowniku\n",
    "maxlen = 200  # Maksymalna długość tekstu\n",
    "embedding_dim = 16 # Wielkość wektora embeddingu (hiperparametr modelu)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = num_words) # Pobieramy dane\n",
    "\n",
    "# Robimy padding komentarzy tak, aby wszystekie miały tę sama długość\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "## Tworzymy model sieci neuronowej z jedn warstwa ukryta z 16 wezlami (taki mamy rozmiar embeddingu)\n",
    "def build_keras_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    # Używamy tutaj regularyzacji L2, aby model nam nie overfitował\n",
    "    model.add(Embedding(input_dim = input_dim, output_dim = output_dim, embeddings_regularizer=l2(0.01)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_keras_model(num_words, embedding_dim)\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "# Predict the sentiment on the test dataset\n",
    "y_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
