{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2164dd26-6d8b-45d6-afcc-1478ddfcde5f",
   "metadata": {},
   "source": [
    "# Zadanie: prognoza vs wartości prawdziwe\n",
    "\n",
    "Pozostaw zbiór testowy jak obecnie. Zmień funkcję do backtestów oraz stworzenia zbiorów treningowego oraz testowego, tak aby nie ucinało pierwszych wartości ze zbioru testowego. Ze zbioru treningowego wyciągnij zbiór walidacyjny. Stwórz coś na wzór ‘siatki hiperparametrów’, tak jak dzieje się w GridSearch (Podpowiedź: Wystarczą pętle). Jako hiperparametry traktuj wartość dla zmiennej look_back oraz liczbę komórek w warstwie (units). Sprawdź zatem wszystkie architektury dla look_back z zakresu od 1 do 12 oraz units z zakresu 1 do 12. Wybierz model, który ma najniższą metrykę RMSE na zbiorze walidacyjnym i zwizualizuj prognozę vs wartości prawdziwe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d352d1a-f449-4594-8b24-ea17ef0ed3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Wczytanie danych\n",
    "dataset = pd.read_csv('airline-passengers.csv')\n",
    "dataset['Month'] = pd.to_datetime(dataset['Month'])\n",
    "dataset.set_index(['Month'], inplace=True)\n",
    "\n",
    "# Podział na zbiór treningowy i testowy\n",
    "train_size = int(len(dataset) * 0.70)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset.iloc[0:train_size], dataset.iloc[train_size:]\n",
    "\n",
    "# Normalizacja danych\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "# Przygotowanie zbioru walidacyjnego (20% zbioru treningowego)\n",
    "val_size = int(train_size * 0.2)\n",
    "val_scaled = train_scaled[:val_size]\n",
    "train_scaled = train_scaled[val_size:]\n",
    "\n",
    "# Funkcja do tworzenia zbiorów danych\n",
    "def create_datasets(data, look_back=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)-look_back):\n",
    "        X.append(data[i:(i+look_back), 0])\n",
    "        y.append(data[i + look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Funkcja do budowy modelu\n",
    "def build_model(look_back, units):\n",
    "    model = Sequential([\n",
    "        Input(shape=(look_back, 1), name='input_layer'),\n",
    "        LSTM(units, activation='tanh'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Przebudowana funkcja backtestu\n",
    "def backtest(model, train_data, test_data, look_back, scaler):\n",
    "    # Prognoza dla danych treningowych\n",
    "    train_predict = model.predict(train_data)\n",
    "    train_predict = scaler.inverse_transform(train_predict)\n",
    "    train_actual = scaler.inverse_transform([train_data[:, -1, 0]])\n",
    "    \n",
    "    # Prognoza dla danych testowych\n",
    "    test_predict = model.predict(test_data)\n",
    "    test_predict = scaler.inverse_transform(test_predict)\n",
    "    test_actual = scaler.inverse_transform([test_data[:, -1, 0]])\n",
    "    \n",
    "    # Obliczenie RMSE\n",
    "    train_rmse = np.sqrt(mean_squared_error(train_actual[0], train_predict[:,0]))\n",
    "    test_rmse = np.sqrt(mean_squared_error(test_actual[0], test_predict[:,0]))\n",
    "    \n",
    "    # Przygotowanie danych do wizualizacji\n",
    "    train_predict_plot = np.empty_like(dataset.values)\n",
    "    train_predict_plot[:, :] = np.nan\n",
    "    train_predict_plot[val_size+look_back:len(train_predict)+val_size+look_back, :] = train_predict\n",
    "    \n",
    "    test_predict_plot = np.empty_like(dataset.values)\n",
    "    test_predict_plot[:, :] = np.nan\n",
    "    test_predict_plot[len(train_predict)+(look_back*2)+val_size+1:len(dataset)-1, :] = test_predict\n",
    "    \n",
    "    # Wizualizacja\n",
    "    plt.figure(figsize=(16,9))\n",
    "    plt.plot(dataset['Passengers'], label='True Values')\n",
    "    plt.plot(pd.DataFrame(train_predict_plot, index=dataset.index), label='Training Predictions')\n",
    "    plt.plot(pd.DataFrame(test_predict_plot, index=dataset.index), label='Test Predictions')\n",
    "    plt.legend()\n",
    "    plt.title(f'Model with look_back={look_back} - Train RMSE: {train_rmse:.2f}, Test RMSE: {test_rmse:.2f}')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_rmse, test_rmse\n",
    "\n",
    "# Siatka hiperparametrów\n",
    "best_rmse = float('inf')\n",
    "best_params = {}\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Rozpoczęcie przeszukiwania siatki hiperparametrów...\")\n",
    "for look_back in range(1, 13):\n",
    "    for units in range(1, 13):\n",
    "        print(f\"\\nTesting look_back={look_back}, units={units}...\")\n",
    "        \n",
    "        # Przygotowanie danych\n",
    "        X_train, y_train = create_datasets(train_scaled, look_back)\n",
    "        X_val, y_val = create_datasets(val_scaled, look_back)\n",
    "        X_test, y_test = create_datasets(test_scaled, look_back)\n",
    "        \n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], look_back, 1))\n",
    "        X_val = np.reshape(X_val, (X_val.shape[0], look_back, 1))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], look_back, 1))\n",
    "        \n",
    "        # Budowa i trening modelu\n",
    "        model = build_model(look_back, units)\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=100,\n",
    "            batch_size=1,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Ocena na zbiorze walidacyjnym\n",
    "        val_predict = model.predict(X_val)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, val_predict))\n",
    "        \n",
    "        # Zapamiętanie najlepszego modelu\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_params = {\n",
    "                'look_back': look_back,\n",
    "                'units': units,\n",
    "                'model': model,\n",
    "                'history': history\n",
    "            }\n",
    "        \n",
    "        results.append({\n",
    "            'look_back': look_back,\n",
    "            'units': units,\n",
    "            'val_rmse': val_rmse,\n",
    "            'train_rmse': np.sqrt(history.history['loss'][-1]),\n",
    "            'val_loss': history.history['val_loss'][-1]\n",
    "        })\n",
    "\n",
    "# Konwersja wyników do DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nPrzeszukiwanie zakończone w %.2f sekund.\" % (time.time() - start_time))\n",
    "print(\"\\nNajlepsze parametry:\", {k: v for k, v in best_params.items() if k != 'model' and k != 'history'})\n",
    "\n",
    "# Ocena najlepszego modelu na zbiorze testowym\n",
    "best_look_back = best_params['look_back']\n",
    "best_units = best_params['units']\n",
    "best_model = best_params['model']\n",
    "\n",
    "X_test_best, y_test_best = create_datasets(test_scaled, best_look_back)\n",
    "X_test_best = np.reshape(X_test_best, (X_test_best.shape[0], best_look_back, 1))\n",
    "\n",
    "test_predict = best_model.predict(X_test_best)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_best, test_predict))\n",
    "print(f\"\\nRMSE na zbiorze testowym dla najlepszego modelu: {test_rmse:.4f}\")\n",
    "\n",
    "# Wizualizacja krzywej uczenia dla najlepszego modelu\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(best_params['history'].history['loss'], label='Train Loss')\n",
    "plt.plot(best_params['history'].history['val_loss'], label='Validation Loss')\n",
    "plt.title(f'Training History (look_back={best_look_back}, units={best_units})')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Wizualizacja najlepszego modelu\n",
    "backtest(best_model,\n",
    "         np.reshape(create_datasets(train_scaled, best_look_back)[0], \n",
    "         X_test_best,\n",
    "         best_look_back,\n",
    "         scaler)\n",
    "\n",
    "# Zapis wyników do pliku\n",
    "results_df.to_csv('grid_search_results.csv', index=False)\n",
    "print(\"\\nWyniki przeszukiwania zapisane w pliku 'grid_search_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf189b-98e1-497b-a1c9-98f280a2d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Wyłącz ostrzeżenia Keras\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# 1. Przygotowanie danych\n",
    "def prepare_data():\n",
    "    dataset = pd.read_csv('airline-passengers.csv')\n",
    "    dataset['Month'] = pd.to_datetime(dataset['Month'])\n",
    "    dataset.set_index(['Month'], inplace=True)\n",
    "    \n",
    "    train_size = int(len(dataset) * 0.70)\n",
    "    train, test = dataset.iloc[0:train_size], dataset.iloc[train_size:]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    test_scaled = scaler.transform(test)\n",
    "    \n",
    "    val_size = int(train_size * 0.2)\n",
    "    val_scaled = train_scaled[:val_size]\n",
    "    train_scaled = train_scaled[val_size:]\n",
    "    \n",
    "    return train_scaled, val_scaled, test_scaled, scaler, dataset\n",
    "\n",
    "# 2. Funkcje pomocnicze\n",
    "def create_datasets(data, look_back=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)-look_back):\n",
    "        X.append(data[i:(i+look_back), 0])\n",
    "        y.append(data[i + look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model(look_back, units):\n",
    "    model = Sequential([\n",
    "        Input(shape=(look_back, 1)),\n",
    "        LSTM(units, activation='tanh'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# 3. Optymalizacja przeszukiwania siatki\n",
    "def optimize_parameters():\n",
    "    train_scaled, val_scaled, _, _, _ = prepare_data()\n",
    "    \n",
    "    results = []\n",
    "    best_rmse = float('inf')\n",
    "    best_params = {}\n",
    "    \n",
    "    # Ograniczony zakres dla demonstracji\n",
    "    for look_back in [3, 6, 9, 12]:  # Zamiast range(1,13)\n",
    "        for units in [4, 8, 12]:      # Zamiast range(1,13)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            X_train, y_train = create_datasets(train_scaled, look_back)\n",
    "            X_val, y_val = create_datasets(val_scaled, look_back)\n",
    "            \n",
    "            X_train = X_train.reshape(-1, look_back, 1)\n",
    "            X_val = X_val.reshape(-1, look_back, 1)\n",
    "            \n",
    "            model = build_model(look_back, units)\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=50,  # Zmniejszona liczba epok\n",
    "                batch_size=8,  # Zwiększony batch size\n",
    "                validation_data=(X_val, y_val),\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            val_rmse = np.sqrt(history.history['val_loss'][-1])\n",
    "            results.append({\n",
    "                'look_back': look_back,\n",
    "                'units': units,\n",
    "                'val_rmse': val_rmse,\n",
    "                'time': time.time() - start_time\n",
    "            })\n",
    "            \n",
    "            if val_rmse < best_rmse:\n",
    "                best_rmse = val_rmse\n",
    "                best_params = {\n",
    "                    'look_back': look_back,\n",
    "                    'units': units,\n",
    "                    'model': model\n",
    "                }\n",
    "            \n",
    "            print(f\"look_back={look_back}, units={units}, val_rmse={val_rmse:.4f}, time={results[-1]['time']:.1f}s\")\n",
    "    \n",
    "    return pd.DataFrame(results), best_params\n",
    "\n",
    "# 4. Główna część skryptu\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Rozpoczęcie optymalizacji...\")\n",
    "    results_df, best_params = optimize_parameters()\n",
    "    \n",
    "    print(\"\\nNajlepsze parametry:\")\n",
    "    print(f\"look_back: {best_params['look_back']}\")\n",
    "    print(f\"units: {best_params['units']}\")\n",
    "    print(f\"val_rmse: {results_df['val_rmse'].min():.4f}\")\n",
    "    \n",
    "    # Ocena na zbiorze testowym\n",
    "    _, _, test_scaled, scaler, dataset = prepare_data()\n",
    "    look_back = best_params['look_back']\n",
    "    \n",
    "    X_test, y_test = create_datasets(test_scaled, look_back)\n",
    "    X_test = X_test.reshape(-1, look_back, 1)\n",
    "    \n",
    "    test_predict = best_params['model'].predict(X_test)\n",
    "    test_predict = scaler.inverse_transform(test_predict)\n",
    "    y_test = scaler.inverse_transform([y_test])[0]\n",
    "    \n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_predict))\n",
    "    print(f\"\\nTest RMSE: {test_rmse:.4f}\")\n",
    "    \n",
    "    # Wizualizacja\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dataset.index[-len(y_test):], y_test, label='True Values')\n",
    "    plt.plot(dataset.index[-len(test_predict):], test_predict, label='Predictions')\n",
    "    plt.title(f\"Best Model (look_back={look_back}, units={best_params['units']})\\nTest RMSE: {test_rmse:.4f}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Zapis wyników\n",
    "    results_df.to_csv('optimization_results.csv', index=False)\n",
    "    print(\"\\nWyniki zapisane w optimization_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
